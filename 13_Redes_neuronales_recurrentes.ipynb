{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "13-Redes-neuronales-recurrentes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK8eUOQU7q2V",
        "colab_type": "text"
      },
      "source": [
        "# 13. Redes Neuronales recurrentes\n",
        "[**Python Deep Learning** Introducción práctica con Keras y TensorFlow 2. Jordi Torres. Editorial Marcombo ISBN: 9788426728289 ](https://www.marcombo.com/python-deep-learning-9788426728289/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Importar TensorFlow 2.0  y otras librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWrJnG1jZd00",
        "colab_type": "code",
        "outputId": "5afd97d8-3a4d-4087-a9e6-b946bd0549d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Descarga del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pD_55cOxLkAb",
        "outputId": "240a44d7-e047-4438-8b4d-668b7ba5cb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "path_to_fileDL = tf.keras.utils.get_file('DL-Introduccion-practica-con-Keras-1aParte.txt', 'https://raw.githubusercontent.com/jorditorresBCN/Deep-Learning-Introduccion-practica-con-Keras/master/DeepLearning-Introduccion-practica-con-Keras-PRIMERA-PARTE.txt')  \n",
        "\n",
        "#path_to_fileDL = tf.keras.utils.get_file('Shakespear.txt', 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt')\n",
        "\n",
        "text = open(path_to_fileDL, 'rb').read().decode(encoding='utf-8')\n",
        "print('Longitud del texto:        {} carácteres'.format(len(text)))\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "print ('El texto está compuesto de estos {} carácteres:'.format(len(vocab)))\n",
        "print (vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/jorditorresBCN/Deep-Learning-Introduccion-practica-con-Keras/master/DeepLearning-Introduccion-practica-con-Keras-PRIMERA-PARTE.txt\n",
            "204800/203286 [==============================] - 0s 0us/step\n",
            "Longitud del texto:        203251 carácteres\n",
            "El texto está compuesto de estos 92 carácteres:\n",
            "['\\n', '\\r', ' ', '!', '\"', '#', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xad', 'ÿ', 'Š', '‡', '…']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "outputId": "a2fe0ad2-6286-432e-b1fe-1dad86e5e94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "for char,_ in zip(char2idx, range(len(vocab))):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '#' :   5,\n",
            "  '%' :   6,\n",
            "  \"'\" :   7,\n",
            "  '(' :   8,\n",
            "  ')' :   9,\n",
            "  '*' :  10,\n",
            "  '+' :  11,\n",
            "  ',' :  12,\n",
            "  '-' :  13,\n",
            "  '.' :  14,\n",
            "  '/' :  15,\n",
            "  '0' :  16,\n",
            "  '1' :  17,\n",
            "  '2' :  18,\n",
            "  '3' :  19,\n",
            "  '4' :  20,\n",
            "  '5' :  21,\n",
            "  '6' :  22,\n",
            "  '7' :  23,\n",
            "  '8' :  24,\n",
            "  '9' :  25,\n",
            "  ':' :  26,\n",
            "  ';' :  27,\n",
            "  '<' :  28,\n",
            "  '=' :  29,\n",
            "  '>' :  30,\n",
            "  '?' :  31,\n",
            "  '@' :  32,\n",
            "  'A' :  33,\n",
            "  'B' :  34,\n",
            "  'C' :  35,\n",
            "  'D' :  36,\n",
            "  'E' :  37,\n",
            "  'F' :  38,\n",
            "  'G' :  39,\n",
            "  'H' :  40,\n",
            "  'I' :  41,\n",
            "  'J' :  42,\n",
            "  'K' :  43,\n",
            "  'L' :  44,\n",
            "  'M' :  45,\n",
            "  'N' :  46,\n",
            "  'O' :  47,\n",
            "  'P' :  48,\n",
            "  'Q' :  49,\n",
            "  'R' :  50,\n",
            "  'S' :  51,\n",
            "  'T' :  52,\n",
            "  'U' :  53,\n",
            "  'V' :  54,\n",
            "  'W' :  55,\n",
            "  'X' :  56,\n",
            "  'Y' :  57,\n",
            "  '[' :  58,\n",
            "  ']' :  59,\n",
            "  '_' :  60,\n",
            "  'a' :  61,\n",
            "  'b' :  62,\n",
            "  'c' :  63,\n",
            "  'd' :  64,\n",
            "  'e' :  65,\n",
            "  'f' :  66,\n",
            "  'g' :  67,\n",
            "  'h' :  68,\n",
            "  'i' :  69,\n",
            "  'j' :  70,\n",
            "  'k' :  71,\n",
            "  'l' :  72,\n",
            "  'm' :  73,\n",
            "  'n' :  74,\n",
            "  'o' :  75,\n",
            "  'p' :  76,\n",
            "  'q' :  77,\n",
            "  'r' :  78,\n",
            "  's' :  79,\n",
            "  't' :  80,\n",
            "  'u' :  81,\n",
            "  'v' :  82,\n",
            "  'w' :  83,\n",
            "  'x' :  84,\n",
            "  'y' :  85,\n",
            "  'z' :  86,\n",
            "  '\\xad':  87,\n",
            "  'ÿ' :  88,\n",
            "  'Š' :  89,\n",
            "  '‡' :  90,\n",
            "  '…' :  91,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML-DuUyi_aHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l1VKcQHcymwb",
        "outputId": "ae9f8163-e3ba-4e8d-d5e8-9963ee511479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print ('texto: {}'.format(repr(text[:50])))\n",
        "print ('{}'.format(repr(text_as_int[:50])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texto: 'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fun'\n",
            "array([48, 78, 75, 72, 75, 67, 75,  1,  0, 37, 74,  2, 17, 25, 21, 19, 12,\n",
            "        2, 41, 79, 61, 61, 63,  2, 33, 79, 69, 73, 75, 82,  2, 76, 81, 62,\n",
            "       72, 69, 63, 75,  2, 51, 65, 67, 81, 74, 64, 61,  2, 38, 81, 74])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Preparar los datos para entrenar la RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0UHJDA39zf-O",
        "colab": {}
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "seq_length = 100\n",
        " \n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4hkDU3i7ozi",
        "outputId": "359c3657-54a9-4bee-c284-d2ce5e74b3a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for item in sequences.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fundacion, el tercer libro de la saga de la Fundacion '\n",
            "'(o el decimotercero segun otras fuentes, este es un tema de debate). En Segunda Fundacion aparece por'\n",
            "' primera vez Arkady Darell, uno de los principales personajes de la parte final de la saga. En su pri'\n",
            "'mera escena, Arkady, que tiene 14 anos, esta haciendo sus tareas escolares. En concreto, una redaccio'\n",
            "'n que lleva por titulo ?El Futuro del Plan Sheldon?. Para hacer la redaccion, Arkady esta utilizando '\n",
            "'un ?transcriptor?,un dispositivo que convierte su voz en palabras escritas. Este tipo de dispositivo,'\n",
            "' que para Isaac Asimov era ciencia ficcion en 1953, lo tenemos al alcance de la mano en la mayoria de'\n",
            "' nuestros smartphones, y el Deep Learning es uno de los responsables de que ya tengamos este tipo de '\n",
            "'aplicaciones, siendo la tecnologia otro de ellos.En la actualidad disponemos de GPUs (Graphics Proces'\n",
            "'sor Units), que solo cuestan alrededor de 100 euros, que estarian en la lista del Top500 hace unos po'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNbw-iR0ymwj",
        "outputId": "62cb263e-b815-4e6f-e5ee-4de68ed4e1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fundacion, el tercer libro de la saga de la Fundacion'\n",
            "Target data: 'rologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fundacion, el tercer libro de la saga de la Fundacion '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eBu9WZG84i0",
        "outputId": "e1ba7b19-33a8-4df0-c7a9-7914bce17b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2pGotuNzf-S",
        "outputId": "0a08cc7c-6684-4a88-a592-f3a527284642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print (dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "### Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zHT8cLh7EAsg",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwsrpOik5zhv",
        "outputId": "98e212ec-f9c6-4ba0-92d6-861fd9178d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fdbc0acd898>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpHGXDMcZ0Zt",
        "colab_type": "code",
        "outputId": "8d448fdd-ffad-417d-e492-ebe27d8ac17e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           23552     \n",
            "_________________________________________________________________\n",
            "unified_lstm (UnifiedLSTM)   (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 92)            94300     \n",
            "=================================================================\n",
            "Total params: 5,364,828\n",
            "Trainable params: 5,364,828\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "outputId": "dfab0fc7-3326-4491-8318-07d1fbdb473b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(\"Input:\", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
        "  print(\"Target:\", target_example_batch.shape, \"# (batch_size, sequence_length)\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: (64, 100) # (batch_size, sequence_length)\n",
            "Target: (64, 100) # (batch_size, sequence_length)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udE_6eTldMhB",
        "colab_type": "code",
        "outputId": "c59f76ef-b449-4143-e1d0-87c442727fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):  \n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\"Prediction: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:  (64, 100, 92) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices_characters = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqFMUQc_UFgM",
        "outputId": "9ed0d2be-20c8-4089-de47-26c3f97f5915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "sampled_indices_characters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 89, 85, 20,  6, 73, 54, 40,  7, 57,  3, 67, 81, 47, 19, 14, 81,\n",
              "       35, 78, 37, 90, 34, 50, 46, 35,  7, 73,  4, 59, 75,  6, 48, 28, 82,\n",
              "       14, 78,  2, 68, 72, 23, 87, 66, 70, 19, 78, 38, 48, 65,  6, 36, 87,\n",
              "       66, 69, 76, 57,  2, 89, 54, 26, 57, 37, 12,  1, 47,  0, 51, 52, 18,\n",
              "       48, 87,  9, 48, 55, 67, 79, 22, 54, 53, 49,  2, 17, 76, 40, 32, 60,\n",
              "       59, 22, 32, 82, 47, 35, 59, 66, 86,  8, 39, 60, 61, 68, 40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDl1_Een6rL0",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ieSJdchZggUj"
      },
      "source": [
        "Configurar el *checkpoints*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6fWTriUZP-n",
        "colab": {}
      },
      "source": [
        " # directorio\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# nombre fichero\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "*Training*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-hmKjYVoll",
        "outputId": "b6b61770-7df4-4fb4-9ef4-aa8162808ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS=50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "31/31 [==============================] - 3s 107ms/step - loss: 3.2626\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 3s 82ms/step - loss: 2.7938\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 3s 81ms/step - loss: 2.4345\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 2.2087\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 2.0815\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 1.9603\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 1.8362\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 1.7192\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 3s 88ms/step - loss: 1.6094\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 1.5072\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 3s 87ms/step - loss: 1.4165\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 3s 88ms/step - loss: 1.3356\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 3s 88ms/step - loss: 1.2693\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 3s 89ms/step - loss: 1.2046\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 3s 88ms/step - loss: 1.1445\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 3s 88ms/step - loss: 1.0885\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 3s 88ms/step - loss: 1.0371\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.9919\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 0.9482\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.9019\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.8546\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.8130\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.7694\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.7322\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.6897\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.6468\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 3s 82ms/step - loss: 0.6114\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.5719\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.5349\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.4936\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.4589\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.4301\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 0.4010\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 3s 82ms/step - loss: 0.3739\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.3510\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.3335\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.3160\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.2952\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.2707\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 3s 83ms/step - loss: 0.2473\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.2284\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 3s 84ms/step - loss: 0.2078\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.1907\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.1738\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 0.1572\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 0.1446\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 0.1352\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.1224\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 3s 86ms/step - loss: 0.1096\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 3s 85ms/step - loss: 0.0976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "### Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk2WJ2-XjkGz",
        "outputId": "65cc78fc-d194-44d3-acb2-1d8cf2c8d2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_50'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "outputId": "575cb534-a089-4eaf-f894-ca056b623177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fdb4a6a0be0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 500\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "\n",
        "\n",
        "  temperature = 0.5\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "outputId": "e462b01b-0462-459c-cd75-dd57e33509f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"Alcohol \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alcohol \n",
            "\r\n",
            "\n",
            "EBUNK9‡+[I**JJJxJK‡@'D]+k'J[‡!9‡DBX*X*WOIB­xRM‡]#WJ['IW\"@‡Wx@I%WGNYI%‡#xY[‡[%B[DIK‡]AQBJ/DB!W\"YYR‡?'IWQ%*YJII­H+B3Dx‡[A‡NQY*­5JQ+XIJwJWWJ\"N7D]YBCR‡YI‡%NHDK\n",
            "U@[x3I­<L!wJ'N9INzw@HB\"9J%>L…#JQJB#R<[L\n",
            "G%IhBD6JK'#]UHIVIY[BTJ<HPN]Y!'ÿ[!‡%*‡]RWJ%Y[D[R%H!JYUIH*I[[4kWQ'B]‡;'Y*[D9%‡WJ<[J*OGNB9\n",
            "JBNwJR'WJBKI#T9Jw?JYI3#[TQ@UJ*UGIQ!@V\"\"V9‡6\"JBG'3K'Yw‡TJ%IAzIQJ+''‡]DA9JJBHJ[F‡FxB\"<I3H9AhW\n",
            "x[JJHY‡BIJB#W<J9]‡2‡QIX\"JJ9GBQx;I‡xH]UD[GJJJ>\"j%x[zHH<JD[w\"*HH+I9Y[DÿD[INBR[DUGY+J#DwH!O[‡\"YX'KWJK\"'IY‡IHJJJ;9>D4‡YV9JKW\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsNst9-GeiZo",
        "colab_type": "code",
        "outputId": "27d0c31a-2c56-4e18-f07c-6beed97ed8dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"modelo\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo se usa para implementar modelos de redes muy grandes.  Para nuestro ejemplo simple, vemos que indica que se requiere mucha expericencambiendo su funcionamos el proceso de aprendizaje de una red neuronal. Ademas veremos algunos de los parametros W y b de tal manera que se minimice la funcion de loss que usaremos para evaluar el grado de error es un espacio de dos cientificos y cientificas de datos y que puedan solurio en la comunidad de programadores ya que permite ample mescampo. Ahora, es el t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFOd58ksem1a",
        "colab_type": "code",
        "outputId": "e6cdf1a2-e3b9-4f06-a022-96e2da3eea87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"activacion\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "activacion si la primera visto y es usaramos una de las predicciones por computador ya se conocian en 1989; tambien los algoritmos fundamentales de Deep Learning puede hacerlo a traves de la como de la UPC model prodecir esta siguiente paralelas Deep Learning ello tenemos otra metrica llamada Sensitivity (o recall) que nos indica como de bien el modelo evita el BSC para referirse a las diferentes actualizaciones de su supercomputador Marenostrum que tanta la provecho de las nuevas se pasan en glabal de da\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}